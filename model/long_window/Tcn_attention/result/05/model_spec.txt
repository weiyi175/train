TCNWithAttentionClassifier(
  (encoder): TCNEncoder(
    (net): Sequential(
      (0): TemporalBlock(
        (conv1): Conv1d(36, 128, kernel_size=(3,), stride=(1,), padding=(2,))
        (chomp1): Chomp1d()
        (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout1): Dropout(p=0.2, inplace=False)
        (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(2,))
        (chomp2): Chomp1d()
        (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout2): Dropout(p=0.2, inplace=False)
        (downsample): Conv1d(36, 128, kernel_size=(1,), stride=(1,))
        (act): ReLU()
      )
      (1): TemporalBlock(
        (conv1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(2,))
        (chomp1): Chomp1d()
        (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout1): Dropout(p=0.2, inplace=False)
        (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(2,))
        (chomp2): Chomp1d()
        (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout2): Dropout(p=0.2, inplace=False)
        (act): ReLU()
      )
      (2): TemporalBlock(
        (conv1): Conv1d(128, 256, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(4,))
        (chomp1): Chomp1d()
        (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout1): Dropout(p=0.2, inplace=False)
        (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(4,))
        (chomp2): Chomp1d()
        (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout2): Dropout(p=0.2, inplace=False)
        (downsample): Conv1d(128, 256, kernel_size=(1,), stride=(1,))
        (act): ReLU()
      )
      (3): TemporalBlock(
        (conv1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(8,))
        (chomp1): Chomp1d()
        (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout1): Dropout(p=0.2, inplace=False)
        (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(16,), dilation=(8,))
        (chomp2): Chomp1d()
        (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout2): Dropout(p=0.2, inplace=False)
        (act): ReLU()
      )
    )
  )
  (attn_pool): AdditiveAttentionPool(
    (proj): Sequential(
      (0): Linear(in_features=256, out_features=192, bias=True)
      (1): Tanh()
      (2): Linear(in_features=192, out_features=1, bias=True)
    )
  )
  (classifier): Sequential(
    (0): Linear(in_features=256, out_features=192, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.4, inplace=False)
    (3): Linear(in_features=192, out_features=2, bias=True)
  )
)